#' @import reshape2
#' @import justinmisc

#' @name sigmoid
#'
#' @param x numeric vector
#'
#' @return sigmoid value for each x
#'
#' @export
sigmoid <- function(x) {
  (1 / (1 + exp(-x)))
}

#' @name aiRlayer
#'
#' @param dim1 row dimensions
#' @param dim2 column dimensions
#'
#' @return generates aiRlayer object of specified dimensions
aiRlayer <- function(dim1,dim2) {
  weights <- matrix(runif(n = dim1*dim2, min = -10, max = 10), nrow = dim1, ncol = dim2)
  change.w <- matrix(0, nrow = dim1, ncol = dim2)
  bias <- runif(n = dim2, min = -5, max = 5)
  change.b <- rep(0, dim2)
  layer <- list(weights, bias, change.w, change.b)
  names(layer) <- c("weights","bias","change.w","change.b")
  class(layer) <- "aiRlayer"
  return(layer)

}
#' @name is.aiRlayer
#'
#' @param x object
#'
#' @return logical value
#'
#' @export
is.aiRlayer <- function(x) inherits(x, "aiRlayer")

#' @name aiRnet
#'
#' @description Builds a network of aiRlayers objects with random weight and bias as a list
#'
#' @param nodes specifies how many nodes in each layer,
#' first value should equal input values, last value should
#' equal classification values
#'
#' @return generates aiRlayer object of specified dimensions
#'
#' @export
aiRnet <- function(nodes) {
  n <- length(nodes)-1
  aiR <- vector("list",n)
  for(i in 1:n){
    aiR[i] <- aiRlayer(nodes[i],nodes[i+1])
  }
  class(aiR) <- "aiRnet"
  return(aiR)
}
#' @name is.aiRnet
#'
#' @param x object
#'
#' @return logical value
#'
#' @export
is.aiRnet <- function(x) inherits(x, "aiRnet")

#' @name aiRsubset
#'
#' @param train.method Method to train data. "Sample" to take a sample of values in data. "Factor"
#' @param sample.size Number between (0-1) that modifies how much of training data is used.
#' @param train.Factor Necessary when train.method set to "Factor". Assign as vector of length 2 in the following form
#' c("column.index/name","factor.level"). Can assign only column name or index but first level is chosen in this case.
#'
#' @return Row numbers to subset
aiRsubset <- function(x, train.method, sample.size, train.Factor = NULL) {
  type.train.method <- c("Sample","Factor")
  if(!is.element(train.method, type.train.method)) {
    stop("train.method must be \"Sample\" or \"Factor\"")
  }
  total.length <- nrow(x)
  if(train.method == "Sample") {
    n <- round(nrow(x)*sample.size,0)
    sample.rows <- sample(as.numeric(row.names(x)),n)
  } else if(train.method == "Factor") {
    if(is.null(train.Factor)) {
      stop("train.Factor must be assigned index or column name when train.method is set to \"Factor\"")
    }
    factor.index <- justinmisc::index.o.coln(vec = train.Factor[1], v.size = 1, v.name = "train.Factor", name.col = colnames(x))
    if(!is.factor(x[,factor.index])) {
      stop(paste("is.factor returned FALSE on column named \"",colnames(x)[factor.index],"\". Change column to a factor.",sep = ""))
    }
    if(length(train.Factor)==1) {
      warning("Only the factor column was specified in train.Factor. Assuming first level of training vector is training set.")
      x <- eval(parse(text = paste(sep = "","x[x$",colnames(x)[factor.index],"==",deparse(levels(x[,factor.index])[1]),",]")))
      n <- round(nrow(x)*sample.size,0)
      sample.rows <- sample(as.numeric(row.names(x)),n)
    } else if(length(train.Factor)==2) {
      if(!any(train.Factor %in% levels(x[,factor.index]))) {
        stop(paste(train.Factor[2]," is not a level of indicated column vector.", sep = ""))
      }
      if(!any(train.Factor %in% x[,factor.index])) {
        stop(paste(train.Factor[2]," is not contained in indicated column vector.", sep = ""))
      }
      x <- eval(parse(text = paste(sep = "","x[x$",colnames(x)[factor.index],"==\"",train.Factor[2],"\",]")))
      n <- round(nrow(x)*sample.size,0)
      sample.rows <- sample(as.numeric(row.names(x)),n)
    } else {
      stop("train.Factor must either be length 1 or 2.")
    }
  }
  sub <- seq(1,total.length) %in% sample.rows
  return(sub)
}

#' @name aiRrun
#'
#' @param data Data frame that contains all named columns needed
#' @param var.classify index or column name of vector that contains classifying values
#' @param layers layers generated by aiRlayer
#' @param cycles Number of cycles done to correct.
#' @param train.method Method to save internal subset of data as the training data. "Sample" to take
#'  a random sample of all rows in data as training set. "Factor" to indicate if you are training
#'  on rows containing a particular factor level.
#' @param sample.size Number between (0-1) that modifies how much of desired train.method data is used.
#' Default set to 0.5. Excluded rows will be used as test examples and not affect aiRlayers.
#' @param batch.size Number indicating how many rows to make batches from training sample. Default set to "all"
#' for no batches to be made.
#' @param train.Factor Necessary when train.method set to "Factor". Assign as vector of length 2 in the following form
#' c("column.index/name","factor.level"). Can assign only column name or index but first level is chosen in this case.
#' @param na.rm remove NAs, default set to TRUE. Function likely to fail with NAs
#'
#' @return data frame of loss and the aiRnet of the training values
#'
#' @export
aiRrun <- function(data,
                   var.classify,
                   layers,
                   cycles = 100,
                   train.method="Sample",
                   sample.size=.5,
                   batch.size="all",
                   train.Factor = NULL,
                   na.rm=TRUE) {
  if(!is.aiRnet(layers)){
    stop("layers must be of class \"aiRnet\"")
  }
  n <- length(layers)             #how many layers
  index <- justinmisc::index.o.coln(vec = var.classify, v.size = 1, v.name = "var.classify", name.col = colnames(data))   #index of classify column
  if(isTRUE(na.rm)) {
    data <- na.exclude(data)
  } else {
    warning("na.rm set to FALSE, NA values not sutable for matrix multiplication. Set NA to place holder value.")
  }
  #How much of data is training, what method to train against
  data.save <- data
  sample.rows <- aiRsubset(x = data, train.method = train.method, sample.size = sample.size, train.Factor = train.Factor)
  classify <- data.save[,index]
  classify.train <- classify[sample.rows]
  classify.test <- classify[!sample.rows]

  data.train <- data[sample.rows,]
  data.train.save <- data.train
  data.test <-  data[!sample.rows,-index]

  loss <- data.frame(train = rep(NA,cycles+1), test = rep(NA,cycles+1))
  rownames(loss) <- as.character(seq(0,cycles))


  data.train <- aiRtransform(data = data.train[,-index], layers = layers)


  #classification matrix
  train.loss <- aiRloss(data = data.train[,-index], layers = layers, classify = classify.train) #squared loss, directional (negative used) loss shows direction it should move.
  loss[1,"train"] <- train.loss$total.loss
  test.loss <- aiRloss(data = aiRtransform(data = data.test,
                                           layers = layers),
                       layers = layers,
                       classify = classify.test)
  loss[1,"test"] <- test.loss$total.loss

  if(is.numeric(batch.size)) {
    if(batch.size<0) {
      stop("batch.size should be a postive integer")
    }
    nbatch <- floor(nrow(data.train)/(batch.size))
  }

  for(k in 1:cycles) {
    if(is.numeric(batch.size)) {
      if(k%%nbatch==1) {
        batches <- aiRbatch(data = data.train.save, batch.size = batch.size)
        data.train <- batches[[k%%nbatch]]
        classify.train <- data.train[,index]
      } else {
        if(k%%nbatch==0) {
          data.train <- batches[[nbatch]]
        } else {
          data.train <- batches[[k%%nbatch]]
        }
        classify.train <- data.train[,index]
      }
    } else if(batch.size=="all") {
      data.train <- data.train.save
    } else {
      stop("batch.size must be a numeric integer or left on default \"all\".")
    }

    train.loss <- aiRloss(data = aiRtransform(data = data.train[,-index]
                                              , layers = layers),
                          layers = layers,
                          classify = classify.train) #squared loss, directional (negative used) loss shows direction it should move.
    test.loss <- aiRloss(data = aiRtransform(data = data.test,
                                             layers = layers),
                         layers = layers,
                         classify = classify.test)
    loss[k+1,"train"] <- train.loss$total.loss
    loss[k+1,"test"] <- test.loss$total.loss
    for(i in 1:length(train.loss$row.loss)) { #for each input, start with loss
      row.work <- train.loss$loss.prop[i,]
      row.work <- sqrt(length(row.work))*row.work*(abs(row.work/(train.loss$row.loss[i])))*(train.loss$row.loss[i]/train.loss$total.loss)
      for(j in n:1) { #use back propigation... record desired changes for each input to each node... record in $change.w $change.b
        w <- mat.opperation(x = abs(layers[[j]]$weights), y = row.work, opperation = "*")
        layers[[j]]$change.w <- layers[[j]]$change.w + w
        b <- abs(layers[[j]]$bias)*row.work
        layers[[j]]$change.b <- layers[[j]]$change.b + b
        new.loss <- apply(w,1,mean)
        row.work <- sqrt(length(new.loss))*new.loss*(abs(new.loss)/sum(abs(new.loss)))*(train.loss$row.loss[i]/train.loss$total.loss)
      }
    }
    last.loss <- layers
    for(i in 1:n) {
      layers[[i]]$change.w <- layers[[i]]$change.w/length(train.loss$row.loss)
      layers[[i]]$change.b <- layers[[i]]$change.b/length(train.loss$row.loss)
      layers[[i]]$weights <- layers[[i]]$weights + layers[[i]]$change.w
      layers[[i]]$bias <- layers[[i]]$bias + layers[[i]]$change.b
      layers[[i]]$change.w <- layers[[i]]$change.w*0
      layers[[i]]$change.b <- layers[[i]]$change.b*0
    }


  }

  aiR <- list(loss, layers)
  names(aiR) <- c("loss","layers")
  class(aiR$layers) <- "aiRnet"

  return(aiR)

}

#' @name aiRtransform
#'
#' @param data data of input values
#' @param layers aiRnet object
#'
#' @return returns layers output of data
#'
#' @export
aiRtransform <- function(data, layers) {
  if(!is.aiRnet(layers)){
    stop("layers must be of class \"aiRnet\"")
  }
  for(i in 1:length(layers)) {   #Transform data through layers
    data <- as.matrix(data )%*%layers[[i]]$weights
    data <- mat.opperation(x = data, y = layers[[i]]$bias, opperation = "+")
    data <- sigmoid(data)
  }
  return(data)
}

#' @name aiRactivation
#'
#' @param data data of input values
#' @param layers aiRnet object
#' @param range.size total amount of data points used to visualize input space.
#'
#' @return returns list of data frames for each aiRlayer in layers. Data frames show
#' activation level.
#'
#' @export
aiRactivation <- function(data, layers, range.size =1000) {
  if(!is.aiRnet(layers)){
    stop("layers must be of class \"aiRnet\"")
  }
  ret.data <- vector("list",length(layers)+1)
  ret.names <- paste("layer",seq(1:length(layers)),sep = "")
  range.work <- range(data[,1])
  range.size <- floor((range.size^(1/(ncol(data)))))
  data.new <- seq(from = floor(range.work[1]),
                  to = ceiling(range.work[2]),
                  by =  diff(c(floor(range.work[1]),ceiling(range.work[2])))/range.size)
  for(i in 2:length(colnames(data))) {
    range.work <- range(data[,i])
    data.work <- seq(from = floor(range.work[1]),
                    to = ceiling(range.work[2]),
                    by =  abs(diff(c(floor(range.work[1]),ceiling(range.work[2])))/range.size))
    data.new <- cbind(data.new,data.work)
  }
  colnames(data.new) <- colnames(data)
  .test <- paste(colnames(data.new)[1]," = data.new[,",1,"]", sep = "")
  for(i in 2:ncol(data.new)) {
    .test <- paste(.test,", ",colnames(data.new)[i]," = data.new[,",i,"]", sep = "")
  }
  eval(parse(text = paste("data.new <- merge_multi(",.test,")")))

  ret.data[[1]] <- data.new
  for(i in 1:length(layers)) {   #Transform data through layers
    data.new <- as.matrix(data.new)%*%layers[[i]]$weights
    data.new <- mat.opperation(x = data.new, y = layers[[i]]$bias,opperation = "+")
    ret.data[[i+1]] <- data.new
    data.new <- sigmoid(data.new)
  }
  names(ret.data) <- c("data_model",ret.names)
  return(ret.data)
}

#' @name aiRclassify
#'
#' @description classifies each observation by taking the max value of the last layer.
#'
#' @param data data of input values
#' @param factor character vector of factors to classify. levels must be in same order as
#' levels presented in original data.
#' @param layers aiRnet object
#'
#' @return returns a list of classification values for each
#' observation along with its activation in the last layer.
#'
#' @export
aiRclassify <- function(data, factor, layers) {
  if(!is.aiRnet(layers)){
    stop("layers must be of class \"aiRnet\"")
  }
  trans <- aiRtransform(data = data, layers = layers)
  node.max <- apply(trans, 1, max)
  classify <- factor[apply(trans==node.max, 1, which)]
  ret <- list(classify, node.max)
  names(ret) <- c("classify","node.max")
  return(ret)
}

#' @name aiRloss
#'
#' @description loss function used on data in training.
#'
#' @param data data of input values
#' @param layers aiRnet object
#' @param classify vector of correct training factors.
#'
#' @return returns aiRloss object which contains;
#' loss.prop: how far away each node was from desired activation,
#' row.loss: the total loss for an observation,
#' total.loss: the total loss for a batch of examples.
#' @export
aiRloss <- function(data, layers, classify) {
  if(!is.aiRnet(layers)){
    stop("layers must be of class \"aiRnet\"")
  }
  class.mat <- diag(nrow = ncol(layers[[length(layers)]]$weights),ncol = ncol(layers[[length(layers)]]$weights))
  correct <- class.mat[classify,]
  loss.prop <- (apply((correct - data),2,neg.exp)) #squared loss, directional (negative used) loss shows direction it should move.
  row.loss <- apply(abs(loss.prop),1,sum)
  total.loss <- sum(row.loss)
  loss <- list(loss.prop, row.loss, total.loss)
  names(loss) <- c("loss.prop","row.loss","total.loss")
  class(loss) <- "aiRloss"
  return(loss)
}

#' @name aiRbatch
#'
#' @description randomly splits data into batches through sample. No repeated samples.
#'
#' @param data data of input values
#' @param batch.size how many observations are used per batch
#'
#' @return a list of data.frames containing random samples of input data.
aiRbatch <- function(data, batch.size) {

  n <- nrow(data)
  n.batch <- floor(n/(batch.size))
  batches <- vector("list",n.batch)
  batch.names <- paste("batch.",seq(1:n.batch),sep = "")
  for(i in 1:n.batch) {
    rows <- seq(1:nrow(data))
    sample.row <- sample(x = rows,size = batch.size)
    batches[[i]] <- data[sample.row,]
    data <- data[-sample.row,]
  }
  names(batches) <- batch.names
  return(batches)
}

#' @name neg.exp
#'
#' @description exponentiates x vector and retains negative values.
#'
#' @param x vector of numeric values
#' @param power value to exponentiate by. default set to 2.
#'
#' @return vector of numeric values equal in length to x
#'
#' @export
neg.exp <- function(x, power = 2) {
  .t <- x<0
  x <- x^(power)
  x[.t] <- -1*x[.t]
  return(x)
}

#' @name mat.opperation
#'
#' @description allows simple operations between matrix and constant vector. column size of x must equal length of y
#'
#' @param x matrix of column size m, row size n
#' @param y vector of length m
#' @param opperation opperation to use: +, -, *, /
#'
#' @return returns n by m matrix where values of y are done onto the columns of x.
#'
#' @export
mat.opperation <- function(x,y, opperation){
  type.opperation <- c("+","-","*","/")
  if(!is.element(opperation, type.opperation)) {
    stop("train.method must be \"+\" or \"-\" or \"*\" or \"/\"")
  }
  if(ncol(x)!=length(y)) {
    stop("column of x must equal length of y.")
  }
  if(opperation=="+"){
    mat <- x+(matrix(rep(y, each = nrow(x)), nrow = nrow(x), ncol = ncol(x)))
  } else if(opperation=="-"){
    mat <- x-(matrix(rep(y, each = nrow(x)), nrow = nrow(x), ncol = ncol(x)))
  } else if(opperation=="*"){
    mat <- x*(matrix(rep(y, each = nrow(x)), nrow = nrow(x), ncol = ncol(x)))
  } else if(opperation=="/"){
    mat <- x/(matrix(rep(y, each = nrow(x)), nrow = nrow(x), ncol = ncol(x)))
  }
  return(mat)
}

#' @name merge_multi
#'
#' @description extends merge function to 3 or more vectors
#'
#' @param ... vectors to be merged. argument names become column names in returned value.
#' If no arguments specified, default column names set to LETTERS set.
#'
#' @return data frame of combinded vectors.
#'
#' @examples
#'
#' merge_multi(x = c(1,2,3), y = c(4,5,6), z = c(7,8,9))
#' merge_multi(c(1,2,3),c(4,5,6),c(7,8,9))
#'
#' @export
merge_multi <- function(...) {
  z <- list(...)
  modes <- mode.type(z)
  if(is.null(names(z))) {
    name <- letters[seq(1,length(z))]
  } else {
    name <- names(z)
  }
  z.new <- interaction(merge(z[[1]],z[[2]]),sep = ",")
  if(length(z)>2) {
    for(i in 3:(length(z))) {
      z.new <- interaction(merge(z.new,z[[i]]),sep = ",")
    }
  }
  z.new <- as.data.frame(z.new)
  colnames(z.new) <- "combind"
  new <- separate(z.new,col = "combind",into = name,sep = ",")
  new <- correct.mode(df = new,mode.vec = modes)
  return(new)
}

#' @name aiRrate
#'
#' @description reports error rate of network
#'
#' @param data data of input values
#' @param factor character vector of factors to classify. levels must be in same order as
#' levels presented in original data.
#' @param layers aiRnet object
#'
#' @return error rate of aiRnet on data
#'
#' @export
aiRrate <- function(data, factor, layers) {
  if(!is.aiRnet(layers)){
    stop("layers must be of class \"aiRnet\"")
  }
  index <- index.o.coln(vec = factor, v.size = 1, v.name = "factor", name.col = colnames(data))
  data.save <- data
  data <- data[,-index]
  classify <- aiRclassify(data = data, factor = levels(data.save[,index]), layers = layers)
  rate <- 1-(mean(data.save[,index]==classify$classify))
  return(rate)
}

#' @name aiRdevelop
#'
#' @param data Data frame that contains all named columns needed
#' @param var.classify index or column name of vector that contains classifying values
#' @param layers aiRnet object
#' @param train.method Method to save internal subset of data as the training data. "Sample" to take
#'  a random sample of all rows in data as training set. "Factor" to indicate if you are training
#'  on rows containing a particular factor level.
#' @param sample.size Number between (0-1) that modifies how much of desired train.method data is used.
#' Default set to 0.5. Excluded rows will be used as test examples and not affect aiRlayers.
#' @param batch.size Number indicating how many rows to make batches from training sample. Default set to "all"
#' for no batches to be made.
#' @param train.Factor Necessary when train.method set to "Factor". Assign as vector of length 2 in the following form
#' c("column.index/name","factor.level"). Can assign only column name or index but first level is chosen in this case.
#' @param steps Number of classification captures desired to view
#' @param range.size total amount of data points used to visualize input space.
#' @param na.rm remove NAs, default set to TRUE. Function likely to fail with NAs
#'
#' @return tidy data frame of the classification and max.node at each step
#'
#' @export
aiRdevelop <- function(data,
                       var.classify,
                       layers,
                       train.method = "Sample",
                       sample.size = .5,
                       batch.size,
                       train.Factor = NULL,
                       steps = 15,
                       range.size = 1000) {
  if(!is.aiRnet(layers)){
    stop("layers must be of class \"aiRnet\"")
  }
  n <- floor(floor(nrow(data)*sample.size)/batch.size)
  index <- index.o.coln(vec = var.classify, v.size = 1, v.name = "var.classify", name.col = colnames(data))
  data.n.class <- data[,-index]
  space.var <- aiRactivation(data = data.n.class, layers = layers, range.size = range.size)
  space.var <- space.var$data_model
  space.var.save <- space.var
  m.t <- mode.type(space.var)
  for(i in 1:steps) {
    if(i==1){
      step.loss <- aiRrun(data = data, var.classify = var.classify, train.method = train.method,layers = layers, cycles = n, sample.size = sample.size, batch.size = batch.size, train.Factor = train.Factor)
    } else {
      step.loss <- aiRrun(data = data, var.classify = var.classify, train.method = train.method,layers = step.loss$layers, cycles = n, sample.size = sample.size, batch.size = batch.size)
    }
    new <- aiRclassify(data = space.var.save, factor = levels(data[,index]),layers = step.loss$layers)
    step <- interaction(new$classify, new$node.max, sep = "_break_")
    eval(parse(text = paste("space.var$step",i," <- step", sep = "")))
  }

  space.var <- melt(space.var, id.vars = commoncol(space.var, data.n.class), variable.name = "step")
  space.var <- separate(data = as.data.frame(space.var), col = "value", into = c("classify","node.max"),sep = "_break_")
  space.var <- correct.mode(space.var, mode.vec = c(m.t,"Factor","Factor","num"))
  browser()
  space.var$step <- factor(space.var$step, levels=c(paste("step",1:steps, sep = "")))

  # gg <- ggplot(data = space.var, aes(x = x, y = y, color = classify, alpha = node.max,frame = step)) +
  #   geom_point()
  #
  # gganimate(gg)
  return(space.var)
}


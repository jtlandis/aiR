#' @import reshape2
#' @import justinmisc
#' @import stringr

#' @name sigmoid
#'
#' @title sigmoid
#'
#' @param x numeric vector
#'
#' @return sigmoid value for each x
#'
#' @export
sigmoid <- function(x) {
  (1 / (1 + exp(-x)))
}

#' @name aiRlayer
#'
#' @title aiRlayer
#'
#' @param dim1 row dimensions
#' @param dim2 column dimensions
#'
#' @return generates aiRlayer object of specified dimensions
aiRlayer <- function(dim1,dim2) {
  weights <- matrix(runif(n = dim1*dim2, min = -10, max = 10), nrow = dim1, ncol = dim2)
  change.w <- matrix(0, nrow = dim1, ncol = dim2)
  bias <- runif(n = dim2, min = -5, max = 5)
  change.b <- rep(0, dim2)
  layer <- list(weights, bias, change.w, change.b)
  names(layer) <- c("weights","bias","change.w","change.b")
  class(layer) <- "aiRlayer"
  return(layer)

}
#' @name is.aiRlayer
#'
#' @title is.aiRlayer
#'
#' @param x object
#'
#' @return logical value
#'
#' @export
is.aiRlayer <- function(x) inherits(x, "aiRlayer")

#' @name aiRnet
#'
#' @title aiRnet
#'
#' @description Builds a network of aiRlayers objects with random weight and bias as a list
#'
#' @param nodes specifies how many nodes in each layer,
#' first value should equal input values, last value should
#' equal classification values
#'
#' @return generates aiRnet object of specified dimensions
#'
#' @export
aiRnet <- function(nodes) {
  n <- length(nodes)-1
  aiR <- vector("list",n)
  for(i in 1:n){
    aiR[[i]] <- aiRlayer(nodes[i],nodes[i+1])
  }
  class(aiR) <- "aiRnet"
  return(aiR)
}
#' @name is.aiRnet
#'
#' @title is.aiRnet
#'
#' @param x object
#'
#' @return logical value
#'
#' @export
is.aiRnet <- function(x) inherits(x, "aiRnet")

#' @name aiRrun
#'
#' @title aiRrun
#'
#' @param data Data frame that contains all named columns needed
#' @param var.classify index or column name of vector that contains classifying values
#' @param aiRnet aiRnet object generated by aiRnet function
#' @param cycles Number of cycles done to correct.
#' @param train.method Method to save internal subset of data as the training data. "Sample" to take
#'  a random sample of all rows in data as training set. "Factor" to indicate if you are training
#'  on rows containing a particular factor level.
#' @param sample.size Number between (0-1) that modifies how much of desired train.method data is used.
#' Default set to 0.5. Excluded rows will be used as test examples and not affect aiRnet.
#' @param batch.size Number indicating how many rows to make batches from training sample. Default set to "all"
#' for no batches to be made.
#' @param train.Factor Necessary when train.method set to "Factor". Assign as vector of length 2 in the following form
#' c("column.index/name","factor.level"). Can assign only column name or index but first level is chosen in this case.
#' @param test toggle if TRUE if test set loss and error should be reported as well. Default set to FALSE
#' @param na.rm remove NAs, default set to TRUE. Function likely to fail with NAs
#' @param data.return logical determines if data should be returned to the user. Default set to FALSE
#'
#' @return data frame of loss and the aiRnet of the training values. data set used is optional
#'
#' @export
aiRrun <- function(data,
                   var.classify,
                   aiRnet,
                   cycles = 100,
                   train.method="Sample",
                   sample.size=.5,
                   batch.size="all",
                   train.Factor = NULL,
                   test = FALSE,
                   na.rm=TRUE,
                   data.return = FALSE) {
  if(!is.aiRnet(aiRnet)){
    stop("aiRnet must be of class \"aiRnet\"")
  }
  n <- length(aiRnet)             #how many layers
  index <- justinmisc::index.o.coln(vec = var.classify, v.size = 1, v.name = "var.classify", name.col = colnames(data))   #index of classify column
  if(isTRUE(na.rm)) {
    data <- na.exclude(data)
  } else {
    warning("na.rm set to FALSE, NA values not sutable for matrix multiplication. Set NA to place holder value.")
  }
  if(!is.element(test, c(TRUE,FALSE))) {
    stop("test must be either TRUE or FALSE")
  }
  if(!is.element(data.return, c(TRUE,FALSE))) {
    stop("data.return must be either TRUE or FALSE")
  }
  if(!is.factor(data[,index])) {
    stop("var.classify must be assigned a factor vector as a classification variable.")
  } else {
    class.levels <- levels(data[,index])  #variable.. recording levels of classification vector.
  }
  #How much of data is training, what method to train against
  sample.rows <- aiRsubset(x = data, train.method = train.method, sample.size = sample.size, train.Factor = train.Factor)
  if((!test)) {
    aiR <- aiRrun_train(data = data,
                        index = index,
                        n = n,
                        class.levels = class.levels,
                        sample.rows = sample.rows,
                        aiRnet = aiRnet,
                        cycles = cycles,
                        batch.size= batch.size)
  } else {
    if(nrow(data)==sum(sample.rows)) {
      warning("All rows of data are used in training set. No test data will be returned.")
      aiR <- aiRrun_train(data = data,
                          index = index,
                          n = n,
                          class.levels = class.levels,
                          sample.rows = sample.rows,
                          aiRnet = aiRnet,
                          cycles = cycles,
                          batch.size= batch.size)
    } else {
      aiR <- aiRrun_test(data = data,
                         index = index,
                         n = n,
                         class.levels = class.levels,
                         sample.rows = sample.rows,
                         aiRnet = aiRnet,
                         cycles = cycles,
                         batch.size= batch.size)  #slightly longer compute time, calculates training loss
    }
  }
  return(aiR)
}

#' @name aiRsubset
#'
#' @title aiRsubset
#'
#' @param train.method Method to train data. "Sample" to take a sample of values in data. "Factor"
#' @param sample.size Number between (0-1) that modifies how much of training data is used.
#' @param train.Factor Necessary when train.method set to "Factor". Assign as vector of length 2 in the following form
#' c("column.index/name","factor.level"). Can assign only column name or index but first level is chosen in this case.
#'
#' @return Row numbers to subset
aiRsubset <- function(x, train.method, sample.size, train.Factor = NULL) {
  type.train.method <- c("Sample","Factor")
  if(!is.element(train.method, type.train.method)) {
    stop("train.method must be \"Sample\" or \"Factor\"")
  }
  total.length <- nrow(x)
  if(train.method == "Sample") {
    n <- round(nrow(x)*sample.size,0)
    sample.rows <- sample(as.numeric(row.names(x)),n)
  } else if(train.method == "Factor") {
    if(is.null(train.Factor)) {
      stop("train.Factor must be assigned index or column name when train.method is set to \"Factor\"")
    }
    factor.index <- justinmisc::index.o.coln(vec = train.Factor[1], v.size = 1, v.name = "train.Factor", name.col = colnames(x))
    if(!is.factor(x[,factor.index])) {
      stop(paste("is.factor returned FALSE on column named \"",colnames(x)[factor.index],"\". Change column to a factor.",sep = ""))
    }
    if(length(train.Factor)==1) {
      warning("Only the factor column was specified in train.Factor. Assuming first level of training vector is training set.")
      x <- eval(parse(text = paste(sep = "","x[x$",colnames(x)[factor.index],"==",deparse(levels(x[,factor.index])[1]),",]")))
      n <- round(nrow(x)*sample.size,0)
      sample.rows <- sample(as.numeric(row.names(x)),n)
    } else if(length(train.Factor)==2) {
      if(!any(train.Factor %in% levels(x[,factor.index]))) {
        stop(paste(train.Factor[2]," is not a level of indicated column vector.", sep = ""))
      }
      if(!any(train.Factor %in% x[,factor.index])) {
        stop(paste(train.Factor[2]," is not contained in indicated column vector.", sep = ""))
      }
      x <- eval(parse(text = paste(sep = "","x[x$",colnames(x)[factor.index],"==\"",train.Factor[2],"\",]")))
      n <- round(nrow(x)*sample.size,0)
      sample.rows <- sample(as.numeric(row.names(x)),n)
    } else {
      stop("train.Factor must either be length 1 or 2.")
    }
  }
  sub <- seq(1,total.length) %in% sample.rows
  return(sub)
}

#' @name aiRrun_train
#'
#' @title aiRrun_train
#'
#' @description body of aiRrun. Only training data subset and its loss calculations
#'
#' @param data Data frame that contains all named columns needed
#' @param index index of vector that contains classifying values
#' @param n number of layers in aiRnet
#' @param class.levels recorded classification levels
#' @param sample.rows logical vector generated by aiRsubset for data sampling
#' @param aiRnet aiRnet object generated by aiRnet function
#' @param cycles Number of cycles done to correct.
#' @param batch.size Number indicating how many rows to make batches from training sample. Default set to "all"
#' for no batches to be made.
#' @param data.return logical determines if data should be returned to the user. Default set to FALSE
#'
#' @return loss, aiRnet, traing.data
aiRrun_train <- function(data,
                         index,
                         n,
                         class.levels,
                         sample.rows,
                         aiRnet,
                         cycles = 100,
                         batch.size = "all",
                         data.return = FALSE) {
  classify <- data[,index]
  classify.train <- classify[sample.rows]

  data.train <- data[sample.rows,]
  data.train.save <- data.train

  loss <- data.frame(train = rep(NA,cycles),train.error = rep(NA,cycles), train.fails = rep(NA,cycles))
  rownames(loss) <- as.character(seq(1,cycles))

  if(is.numeric(batch.size)) {
    if(batch.size<0) {
      stop("batch.size should be a postive integer")
    }
    nbatch <- floor(nrow(data.train)/(batch.size))
  }
browser()
  for(k in 1:cycles) {
    if(is.numeric(batch.size)) {
      if(k%%nbatch==1) {
        batches <- aiRbatch(data = data.train.save, batch.size = batch.size)
        data.train <- batches[[k%%nbatch]]
        classify.train <- data.train[,index]
      } else {
        if(k%%nbatch==0) {
          data.train <- batches[[nbatch]]
        } else {
          data.train <- batches[[k%%nbatch]]
        }
        classify.train <- data.train[,index]
      }
    } else if(batch.size=="all") {
      data.train <- data.train.save
    } else {
      stop("batch.size must be a numeric integer or left on default \"all\".")
    }

    train.loss <- aiRloss(data = aiRtransform(data = data.train[,-index],
                                              aiRnet = aiRnet),
                          class.levels = class.levels,
                          classify = classify.train) #squared loss, directional (negative used) loss shows direction it should move.
    if(k==1){
      warning <- TRUE
    } else if((loss$train.error[k-1]!=0)) {
      warning <- FALSE
    }
    train.rate <- aiRrate(data = data.train,
                          factor = index,
                          aiRnet = aiRnet,
                          report.class = F,
                          warning = warning)
    loss$train.error[k] <- train.rate$MeanError
    loss$train.fails[k] <- train.rate$failed.instances
    loss$train[k] <- train.loss$total.loss
    #for(i in 1:length(train.loss$row.loss)) { #for each input, start with loss,
      aiRnet <- aiRrowdelta(loss.prop = train.loss$loss.prop,  #Does average of all observations to speed up computation time.
                            total.loss = train.loss$total.loss,
                            aiRnet = aiRnet,
                            n = n)
    #}
    last.loss <- aiRnet
    aiRnet <- aiRfresh(aiRnet = aiRnet,rows = length(train.loss$row.loss), n = n)
  }

  if(data.return) {
    aiR <- list(loss, aiRnet, data.train.save)
    names(aiR) <- c("loss","aiRnet", "training.data")
  } else {
    aiR <- list(loss, aiRnet)
    names(aiR) <- c("loss","aiRnet")
  }
  class(aiR$aiRnet) <- "aiRnet"

  return(aiR)
}

#' @name aiRrun_test
#'
#' @title aiRrun_test
#'
#' @description body of aiRrun. Includes test data subset and its loss calculations
#'
#' @param data Data frame that contains all named columns needed
#' @param index index of vector that contains classifying values
#' @param n number of layers in aiRnet
#' @param class.levels recorded classification levels
#' @param sample.rows logical vector generated by aiRsubset for data sampling
#' @param aiRnet aiRnet object generated by aiRnet function
#' @param cycles Number of cycles done to correct.
#' @param batch.size Number indicating how many rows to make batches from training sample. Default set to "all"
#' for no batches to be made.
#' @param data.return logical determines if data should be returned to the user. Default set to FALSE
#'
#' @return loss, aiRnet, traing.data and test.data
aiRrun_test <- function(data,
                        index,
                        n,
                        class.levels,
                        sample.rows,
                        aiRnet,
                        cycles = 100,
                        batch.size = "all",
                        data.return = FALSE) {
  classify <- data[,index]
  classify.train <- classify[sample.rows]
  classify.test <- classify[!sample.rows]

  data.train <- data[sample.rows,]
  data.train.save <- data.train
  data.test <-  data[!sample.rows,]

  loss <- data.frame(train = rep(NA,cycles),train.error = rep(NA,cycles), train.fails = rep(NA,cycles), test = rep(NA,cycles), test.error = rep(NA,cycles), test.fails = rep(NA,cycles))
  rownames(loss) <- as.character(seq(1,cycles))

  if(is.numeric(batch.size)) {
    if(batch.size<0) {
      stop("batch.size should be a postive integer")
    }
    nbatch <- floor(nrow(data.train)/(batch.size))
  }

  for(k in 1:cycles) {
    if(is.numeric(batch.size)) {
      if(k%%nbatch==1) {
        batches <- aiRbatch(data = data.train.save, batch.size = batch.size)
        data.train <- batches[[k%%nbatch]]
        classify.train <- data.train[,index]
      } else {
        if(k%%nbatch==0) {
          data.train <- batches[[nbatch]]
        } else {
          data.train <- batches[[k%%nbatch]]
        }
        classify.train <- data.train[,index]
      }
    } else if(batch.size=="all") {
      data.train <- data.train.save
    } else {
      stop("batch.size must be a numeric integer or left on default \"all\".")
    }

    if(k==1){
      warning <- TRUE
    } else if((loss$train.error[k-1]!=0)) {
      warning <- FALSE
    }
    train.loss <- aiRloss(data = aiRtransform(data = data.train[,-index],
                                              aiRnet = aiRnet),
                          class.levels = class.levels,
                          classify = classify.train) #squared loss, directional (negative used) loss shows direction it should move.
    test.loss <- aiRloss(data = aiRtransform(data = data.test[,-index],
                                             aiRnet = aiRnet),
                         class.levels = class.levels,
                         classify = classify.test)
    loss$train[k] <- train.loss$total.loss
    train.rate <- aiRrate(data = data.train,
                          factor = index,
                          aiRnet = aiRnet,
                          report.class = F,
                          warning = warning)
    loss$train.error[k] <- train.rate$MeanError
    loss$train.fails[k] <- train.rate$failed.instances
    loss$test[k] <- test.loss$total.loss
    test.rate <- aiRrate(data = data.test,
                          factor = index,
                          aiRnet = aiRnet,
                         report.class = F,
                         warning = warning)
    loss$test.error[k] <- test.rate$MeanError
    loss$test.fails[k] <- test.rate$failed.instances
    #for(i in 1:length(train.loss$row.loss)) { #for each input, start with loss
      aiRnet <- aiRrowdelta(loss.prop = train.loss$loss.prop,
                            total.loss = train.loss$total.loss,
                            aiRnet = aiRnet,
                            n = n)
    #}
    last.loss <- aiRnet
    aiRnet <- aiRfresh(aiRnet = aiRnet,rows = length(train.loss$row.loss), n = n)
  }

  if(data.return) {
    aiR <- list(loss, aiRnet, data.train.save, data.test)
    names(aiR) <- c("loss","aiRnet", "training.data", "test.data")
  } else {
    aiR <- list(loss, aiRnet)
    names(aiR) <- c("loss","aiRnet")
  }
  class(aiR$aiRnet) <- "aiRnet"

  return(aiR)
}

#' @name aiRbatch
#'
#' @title aiRbatch
#'
#' @description randomly splits data into batches through sample. No repeated samples.
#'
#' @param data data of input values
#' @param batch.size how many observations are used per batch
#'
#' @return a list of data.frames containing random samples of input data.
aiRbatch <- function(data, batch.size) {

  n <- nrow(data)
  n.batch <- floor(n/(batch.size))
  batches <- vector("list",n.batch)
  batch.names <- paste("batch.",seq(1:n.batch),sep = "")
  for(i in 1:n.batch) {
    rows <- seq(1:nrow(data))
    sample.row <- sample(x = rows,size = batch.size)
    batches[[i]] <- data[sample.row,]
    data <- data[-sample.row,]
  }
  names(batches) <- batch.names
  return(batches)
}

#' @name aiRloss
#'
#' @title aiRloss
#'
#' @description loss function used on data in training.
#'
#' @param data aiRtransform data. data of activation nodes
#' @param class.levels character vector that identifies the
#' ordering of classification factor levels.
#' @param classify vector of correct training factors.
#'
#' @return returns aiRloss object which contains;
#' loss.prop: how far away each node was from desired activation,
#' row.loss: the total loss for an observation,
#' total.loss: the total loss for a batch of examples.
#' @export
aiRloss <- function(data, class.levels, classify) {
  class.mat <- diag(nrow = length(class.levels),ncol = length(class.levels))
  correct <- class.mat[classify,]
  loss.prop <- (apply((correct - data),2,neg.exp)) #squared loss, directional (negative used) loss shows direction it should move.
  row.loss <- apply(abs(loss.prop),1,sum)
  total.loss <- sum(row.loss)
  loss <- list(loss.prop, row.loss, total.loss)
  names(loss) <- c("loss.prop","row.loss","total.loss")
  class(loss) <- "aiRloss"
  return(loss)
}

#' @name aiRrate
#'
#' @title aiRrate
#'
#' @description reports error rate of network
#'
#' @param data data of input values
#' @param factor character vector of factors to classify. levels must be in same order as
#' levels presented in original data.
#' @param aiRnet aiRnet object
#' @param report.class report results from aiRclassify, Default set to FALSE
#' @param warning Suppress warning from aiRclassify that multiple nodes are activated.
#' Default set to TRUE. TRUE: allow warning. FALSE: Suppress warning
#'
#' @return error rate of aiRnet on data and how many activated non uniquely.
#'
#' @export
aiRrate <- function(data, factor, aiRnet, report.class = FALSE, warning = TRUE) {
  if(!is.aiRnet(aiRnet)){
    stop("aiRnet must be of class \"aiRnet\"")
  }
  if(!is.element(report.class, c(TRUE,FALSE))){
    stop("report.class must be logical, either: TRUE or FALSE")
  }
  index <- index.o.coln(vec = factor, v.size = 1, v.name = "factor", name.col = colnames(data))
  class.save <- data[,index]
  data <- data[,-index]
  classify <- aiRclassify(data = data, factor = levels(class.save), aiRnet = aiRnet, warning = warning)
  rate <- 1-(mean(class.save==classify$classify))
  if(report.class) {
    ret <- list(rate, classify$classify, classify$node.max, classify$failed.instances)
    names(ret) <- c("MeanError","classify","node.max", "failed.instances")
  } else {
    ret <- list(rate, classify$failed.instances)
    names(ret) <- c("MeanError","failed.instances")
  }
  return(ret)
}

#' @name aiRclassify
#'
#' @title aiRclassify
#'
#' @description classifies each observation by taking the max value of the last layer.
#'
#' @param data data of input values
#' @param factor character vector of factors to classify. levels must be in same order as
#' levels presented in original data.
#' @param aiRnet aiRnet object
#' @param warning Suppress warning from aiRclassify that multiple nodes are activated.
#' Default set to TRUE. TRUE: allow warning. FALSE: Suppress warning
#'
#' @return returns a list of classification values for each
#' observation along with its activation in the last layer.
#'
#' @export
aiRclassify <- function(data, factor, aiRnet, warning = TRUE) {
  if(!is.aiRnet(aiRnet)){
    stop("aiRnet must be of class \"aiRnet\"")
  }
  trans <- aiRtransform(data = data, aiRnet = aiRnet)
  node.max <- apply(trans, 1, max)
  indexes <- apply(trans==node.max, 1, which)
  if(class(indexes)=="list") {
    if(warning) {
      warning("for at least one observation, more than one node have the same max activation. More training advised.")
    }
    classify <- unlist(lapply(lapply(indexes,FUN = function(x, y = factor) {y[x]}),str_flatten))
    fails <- sum(!classify %in% factor)
  } else {
    classify <- factor[indexes]
  }
  ret <- list(classify, node.max, fails)
  names(ret) <- c("classify","node.max", "failed.instances")
  return(ret)
}

#' @name aiRtransform
#'
#' @title aiRtransform
#'
#' @param data data of input values
#' @param aiRnet aiRnet object
#'
#' @return returns aiRnet output of data
#'
#' @export
aiRtransform <- function(data, aiRnet) {
  if(!is.aiRnet(aiRnet)){
    stop("aiRnet must be of class \"aiRnet\"")
  }
  for(i in 1:length(aiRnet)) {   #Transform data through aiRnet
    data <- as.matrix(data )%*%aiRnet[[i]]$weights
    data <- mat.opperation(x = data, y = aiRnet[[i]]$bias, opperation = "+")
    data <- sigmoid(data)
  }
  return(data)
}

#' @name mat.opperation
#'
#' @title mat.opperation
#'
#' @description allows simple operations between matrix and constant vector. column size of x must equal length of y
#'
#' @param x matrix of column size m, row size n
#' @param y vector of length m
#' @param opperation opperation to use: +, -, *, /
#'
#' @return returns n by m matrix where values of y are done onto the columns of x.
#'
#' @export
mat.opperation <- function(x,y, opperation){
  type.opperation <- c("+","-","*","/")
  if(!is.element(opperation, type.opperation)) {
    stop("train.method must be \"+\" or \"-\" or \"*\" or \"/\"")
  }
  if(ncol(x)!=length(y)) {
    stop("column of x must equal length of y.")
  }
  if(opperation=="+"){
    mat <- sweep(x, 2, y, "+")
  } else if(opperation=="-"){
    mat <- sweep(x, 2, y, "-")
  } else if(opperation=="*"){
    mat <- sweep(x, 2, y, "*")
  } else if(opperation=="/"){
    mat <- sweep(x, 2, y, "/")
  }
  return(mat)
}

#' @name aiRrowdelta
#'
#' @title aiRrowdelta
#'
#' @description back Propagation function
#'
#' @param loss.prop generated from aiRloss
#' @param total.loss generated from aiRloss
#' @param aiRnet aiRnet object
#' @param n length of aiRnet
#'
#' @return new aiRnet object
aiRrowdelta <- function(loss.prop,
                        total.loss,
                        aiRnet,
                        n) {
  #g <- nrow(loss.prop)
  row.work <- apply(loss.prop,2,mean)
  additional.c <- apply(loss.prop*abs(loss.prop),2,mean)/((apply(loss.prop,2,mean)*abs(apply(loss.prop,2,mean))))
  row.work <- additional.c*sqrt(length(row.work))*row.work*(abs(row.work/(total.loss)))
  #row.work <- sqrt(length(loss.prop))*loss.prop*(abs(loss.prop/(total.loss)))
  for(j in n:1) { #use back propigation... record desired changes for each input to each node... record in $change.w $change.b
    w <- mat.opperation(x = abs(aiRnet[[j]]$weights), y = row.work, opperation = "*")
    aiRnet[[j]]$change.w <- aiRnet[[j]]$change.w + w
    b <- abs(aiRnet[[j]]$bias)*row.work
    aiRnet[[j]]$change.b <- aiRnet[[j]]$change.b + b
    new.loss <- apply(w,1,mean)
    #additional.c <- (sum(apply(loss.prop,2,mean)^2))*(sum(abs(apply(loss.prop^2,1,sum)))/(g*(sum(apply(loss$loss.prop^2,1,sum)*abs(apply(loss$loss.prop^2,1,sum))))))
    row.work <- sqrt(length(new.loss))*new.loss*(abs(new.loss)/sum(abs(new.loss)))
  }
  return(aiRnet)
}

#' @name aiRfresh
#'
#' @title aiRfresh
#'
#' @param aiRnet aiRnet object
#' @param rows number of rows in a batch
#' @param n length of aiRnet object
#'
#' @return returns aiRnet object with change.w and change.b added
#' to weights and bias respectively
aiRfresh <- function(aiRnet, rows, n) {
  for(i in 1:n) {
    aiRnet[[i]]$change.w <- aiRnet[[i]]$change.w/rows
    aiRnet[[i]]$change.b <- aiRnet[[i]]$change.b/rows
    aiRnet[[i]]$weights <- aiRnet[[i]]$weights + aiRnet[[i]]$change.w
    aiRnet[[i]]$bias <- aiRnet[[i]]$bias + aiRnet[[i]]$change.b
    aiRnet[[i]]$change.w <- aiRnet[[i]]$change.w*0
    aiRnet[[i]]$change.b <- aiRnet[[i]]$change.b*0
  }
  return(aiRnet)
}

#' @name aiRactivation
#'
#' @title aiRactivation
#'
#' @param data data of input values
#' @param aiRnet aiRnet object
#' @param range.size total amount of data points used to visualize input space.
#'
#' @return returns list of data frames for each aiRlayer in aiRnet. Data frames show
#' activation level.
#'
#' @export
aiRactivation <- function(data, aiRnet, range.size =1000) {
  if(!is.aiRnet(aiRnet)){
    stop("aiRnet must be of class \"aiRnet\"")
  }
  ret.data <- vector("list",length(aiRnet)+1)
  ret.names <- paste("layer",seq(1:length(aiRnet)),sep = "")
  range.work <- range(data[,1])
  range.size <- floor((range.size^(1/(ncol(data)))))
  data.new <- seq(from = floor(range.work[1]),
                  to = ceiling(range.work[2]),
                  by =  diff(c(floor(range.work[1]),ceiling(range.work[2])))/range.size)
  for(i in 2:length(colnames(data))) {
    range.work <- range(data[,i])
    data.work <- seq(from = floor(range.work[1]),
                    to = ceiling(range.work[2]),
                    by =  abs(diff(c(floor(range.work[1]),ceiling(range.work[2])))/range.size))
    data.new <- cbind(data.new,data.work)
  }
  colnames(data.new) <- colnames(data)
  .test <- paste(colnames(data.new)[1]," = data.new[,",1,"]", sep = "")
  for(i in 2:ncol(data.new)) {
    .test <- paste(.test,", ",colnames(data.new)[i]," = data.new[,",i,"]", sep = "")
  }
  eval(parse(text = paste("data.new <- merge_multi(",.test,")")))

  ret.data[[1]] <- data.new
  for(i in 1:length(aiRnet)) {   #Transform data through aiRnet
    data.new <- as.matrix(data.new)%*%aiRnet[[i]]$weights
    data.new <- mat.opperation(x = data.new, y = aiRnet[[i]]$bias,opperation = "+")
    ret.data[[i+1]] <- data.new
    data.new <- sigmoid(data.new)
  }
  names(ret.data) <- c("data_model",ret.names)
  return(ret.data)
}




#' @name neg.exp
#'
#' @title neg.exp
#'
#' @description exponentiates x vector and retains negative values.
#'
#' @param x vector of numeric values
#' @param power value to exponentiate by. default set to 2.
#'
#' @return vector of numeric values equal in length to x
#'
#' @export
neg.exp <- function(x, power = 2) {
  .t <- x<0
  x <- x^(power)
  x[.t] <- -1*x[.t]
  return(x)
}



#' @name merge_multi
#'
#' @title merge_multi
#'
#' @description extends merge function to 3 or more vectors
#'
#' @param ... vectors to be merged. argument names become column names in returned value.
#' If no arguments specified, default column names set to LETTERS set.
#'
#' @return data frame of combinded vectors.
#'
#' @examples
#'
#' merge_multi(x = c(1,2,3), y = c(4,5,6), z = c(7,8,9))
#' merge_multi(c(1,2,3),c(4,5,6),c(7,8,9))
#'
#' @export
merge_multi <- function(...) {
  z <- list(...)
  modes <- mode.type(z)
  if(is.null(names(z))) {
    name <- letters[seq(1,length(z))]
  } else {
    name <- names(z)
  }
  z.new <- interaction(merge(z[[1]],z[[2]]),sep = ",")
  if(length(z)>2) {
    for(i in 3:(length(z))) {
      z.new <- interaction(merge(z.new,z[[i]]),sep = ",")
    }
  }
  z.new <- as.data.frame(z.new)
  colnames(z.new) <- "combind"
  new <- separate(z.new,col = "combind",into = name,sep = ",")
  new <- correct.mode(df = new,mode.vec = modes)
  return(new)
}

#' @name aiRdevelop
#'
#' @title aiRdevelop
#'
#' @param data Data frame that contains all named columns needed
#' @param var.classify index or column name of vector that contains classifying values
#' @param aiRnet aiRnet object
#' @param train.method Method to save internal subset of data as the training data. "Sample" to take
#'  a random sample of all rows in data as training set. "Factor" to indicate if you are training
#'  on rows containing a particular factor level.
#' @param sample.size Number between (0-1) that modifies how much of desired train.method data is used.
#' Default set to 0.5. Excluded rows will be used as test examples and not affect aiRaiRnet.
#' @param batch.size Number indicating how many rows to make batches from training sample. Default set to "all"
#' for no batches to be made.
#' @param train.Factor Necessary when train.method set to "Factor". Assign as vector of length 2 in the following form
#' c("column.index/name","factor.level"). Can assign only column name or index but first level is chosen in this case.
#' @param steps Number of classification captures desired to view
#' @param range.size total amount of data points used to visualize input space.
#' @param na.rm remove NAs, default set to TRUE. Function likely to fail with NAs
#'
#' @return tidy data frame of the classification and max.node at each step
#'
#' @export
aiRdevelop <- function(data,
                       var.classify,
                       aiRnet,
                       train.method = "Sample",
                       sample.size = .5,
                       batch.size,
                       train.Factor = NULL,
                       steps = 15,
                       range.size = 1000) {
  if(!is.aiRnet(aiRnet)){
    stop("aiRnet must be of class \"aiRnet\"")
  }
  n <- floor(floor(nrow(data)*sample.size)/batch.size)
  index <- index.o.coln(vec = var.classify, v.size = 1, v.name = "var.classify", name.col = colnames(data))
  data.n.class <- data[,-index]
  space.var <- aiRactivation(data = data.n.class, aiRnet = aiRnet, range.size = range.size)
  space.var <- space.var$data_model
  space.var.save <- space.var
  m.t <- mode.type(space.var)
  for(i in 1:steps) {
    if(i==1){
      step.loss <- aiRrun(data = data, var.classify = var.classify, train.method = train.method,aiRnet = aiRnet, cycles = n, sample.size = sample.size, batch.size = batch.size, train.Factor = train.Factor)
    } else {
      step.loss <- aiRrun(data = data, var.classify = var.classify, train.method = train.method,aiRnet = step.loss$aiRnet, cycles = n, sample.size = sample.size, batch.size = batch.size)
    }
    new <- aiRclassify(data = space.var.save, factor = levels(data[,index]),aiRnet = step.loss$aiRnet)
    step <- interaction(new$classify, new$node.max, sep = "_break_")
    eval(parse(text = paste("space.var$step",i," <- step", sep = "")))
  }

  space.var <- melt(space.var, id.vars = commoncol(space.var, data.n.class), variable.name = "step")
  space.var <- separate(data = as.data.frame(space.var), col = "value", into = c("classify","node.max"),sep = "_break_")
  space.var <- correct.mode(space.var, mode.vec = c(m.t,"Factor","Factor","num"))
  space.var$step <- factor(space.var$step, levels=c(paste("step",1:steps, sep = "")))

  # gg <- ggplot(data = space.var, aes(x = x, y = y, color = classify, alpha = node.max,frame = step)) +
  #   geom_point()
  #
  # gganimate(gg)
  return(space.var)
}


